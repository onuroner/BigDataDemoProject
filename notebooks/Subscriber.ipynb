{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d634f27-e3a6-4c52-81f2-4b521476cb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pykafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda19ab-288a-4a98-9d1b-aace389feaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fastavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cebe7b7-0ef4-4b84-b004-72e33f4962c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee9fee67-ed3b-4730-a338-ba3f657b35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.functions import to_binary\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n",
    "import json\n",
    "from pykafka import KafkaClient\n",
    "import pyspark.sql.functions as psf\n",
    "from pyspark.sql.types import *\n",
    "import io\n",
    "import fastavro\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00c473-caec-433e-9d17-68d02b211611",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = KafkaClient(hosts=\"broker:29092\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7869fd6-5ec3-4704-835b-8eb5896b6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_topic = \"PurchasedItems\"\n",
    "\n",
    "# Kafka broker adresi\n",
    "kafka_bootstrap_servers = \"broker:29092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2f52fcf-2cf4-4ade-826a-9af51b37aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaRegistryAddr = \"http://schema-registry:8081\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9857bb7-3b8e-48ad-ab84-ecabf5eaa738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-avro_2.12:3.2.0') \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3f90fa6-599f-479a-8fee-4315e0487351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema_from_schema_registry(schema_registry_url, schema_registry_subject):\n",
    "    sr = SchemaRegistryClient({'url': schema_registry_url})\n",
    "    latest_version = sr.get_latest_version(schema_registry_subject)\n",
    "\n",
    "    return sr, latest_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab22bbac-7477-49c3-9107-84882f1342f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema, version = get_schema_from_schema_registry(schemaRegistryAddr, \"PurchasedItems-value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1bd24ef-d365-4ff5-a549-db8e082597c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"type\":\"record\",\"name\":\"PurchasedItemsSchema\",\"fields\":[{\"name\":\"sessionId\",\"type\":\"int\"},{\"name\":\"timestamp\",\"type\":\"string\"},{\"name\":\"userId\",\"type\":\"int\"},{\"name\":\"totalPrice\",\"type\":\"double\"},{\"name\":\"orderId\",\"type\":\"int\"},{\"name\":\"products\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"record\",\"name\":\"products\",\"namespace\":\"Record\",\"fields\":[{\"name\":\"product_id\",\"type\":\"int\"},{\"name\":\"item_count\",\"type\":\"int\"},{\"name\":\"item_price\",\"type\":\"double\"},{\"name\":\"item_discount\",\"type\":\"double\"}]}}}]}'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version.schema.schema_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e0d56159-d66a-4107-ba91-4efb57b43cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream\\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    ".option(\"mode\",\"PERMISSIVE\") \\\n",
    "    .load() \\\n",
    "    .select(from_avro(\"value\", version.schema.schema_str).alias(\"value\")).select(\"value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f2a9406-2225-4365-bb68-4cd37cb2ac09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sessionId: integer (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- totalPrice: double (nullable = true)\n",
      " |-- orderId: integer (nullable = true)\n",
      " |-- products: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- product_id: integer (nullable = false)\n",
      " |    |    |-- item_count: integer (nullable = false)\n",
      " |    |    |-- item_price: double (nullable = false)\n",
      " |    |    |-- item_discount: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a0f972-9e88-40b0-93de-292e0a3578c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_struct = StructType([\n",
    "    StructField(\"sessionId\", IntegerType()),\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"userId\", IntegerType()),\n",
    "    StructField(\"totalPrice\", DoubleType()),\n",
    "    StructField(\"orderId\", IntegerType()) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad17fab-cbed-4bd2-8e74-784575315a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32fd031-5995-43a7-92e3-c175b8b6cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = streaming_df.select(psf.expr(\"substring(value,1,1)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df13e5f-5a94-468e-b53a-44076db070f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFormatSchema = open(\"PurchasedItemsSchema.avsc\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c55c77-5389-4232-bc1b-cf8c472dc365",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = streaming_df\\\n",
    "  .select(from_avro(psf.col(\"value\"), jsonFormatSchema).alias(\"avro\")).select(psf.col(\"avro.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad683847-f4eb-4f27-8479-a1e5ae48d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3de6b3-e595-4b14-abe4-e4a16d571e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "037a6309-f6ec-456d-854b-8c38017bc987",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = streaming_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", \"purchased_items.json\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint-2\") \\\n",
    "    .start()\n",
    "\n",
    "# İşlemi başlat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a11b2a-74ee-40ed-9755-091ea8403bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca11239e-3b4c-4646-9117-badde1297feb",
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = afa4f900-222d-4868-93a4-6c0332375acd, runId = ef13b456-f06a-4e43-b34b-a9adec9e16d3] terminated with exception: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12) (3e248013a1c0 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to purchased_items.json.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$2(FileFormatWriter.scala:252)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Current parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.avro.AvroDataToCatalyst.nullSafeEval(AvroDataToCatalyst.scala:113)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 12 more\nCaused by: java.io.EOFException\n\tat org.apache.avro.io.BinaryDecoder.ensureBounds(BinaryDecoder.java:543)\n\tat org.apache.avro.io.BinaryDecoder.readDouble(BinaryDecoder.java:288)\n\tat org.apache.avro.io.ResolvingDecoder.readDouble(ResolvingDecoder.java:197)\n\tat org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:202)\n\tat org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:161)\n\tat org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:260)\n\tat org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:248)\n\tat org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:180)\n\tat org.apache.avro.generic.GenericDatumReader.readArray(GenericDatumReader.java:299)\n\tat org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:184)\n\tat org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:161)\n\tat org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:260)\n\tat org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:248)\n\tat org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:180)\n\tat org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:161)\n\tat org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:154)\n\tat org.apache.spark.sql.avro.AvroDataToCatalyst.nullSafeEval(AvroDataToCatalyst.scala:99)\n\t... 19 more\n\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = afa4f900-222d-4868-93a4-6c0332375acd, runId = ef13b456-f06a-4e43-b34b-a9adec9e16d3] terminated with exception: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12) (3e248013a1c0 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to purchased_items.json.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$2(FileFormatWriter.scala:252)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Current parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.avro.AvroDataToCatalyst.nullSafeEval(AvroDataToCatalyst.scala:113)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 12 more\nCaused by: java.io.EOFException\n\tat org.apache.avro.io.BinaryDecoder.ensureBounds(BinaryDecoder.java:543)\n\tat org.apache.avro.io.BinaryDecoder.readDouble(BinaryDecoder.java:288)\n\tat org.apache.avro.io.ResolvingDecoder.readDouble(ResolvingDecoder.java:197)\n\tat org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:202)\n\tat org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:161)\n\tat org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:260)\n\tat org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:248)\n\tat org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:180)\n\tat org.apache.avro.generic.GenericDatumReader.readArray(GenericDatumReader.java:299)\n\tat org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:184)\n\tat org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:161)\n\tat org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:260)\n\tat org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:248)\n\tat org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:180)\n\tat org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:161)\n\tat org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:154)\n\tat org.apache.spark.sql.avro.AvroDataToCatalyst.nullSafeEval(AvroDataToCatalyst.scala:99)\n\t... 19 more\n\nDriver stacktrace:"
     ]
    }
   ],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9947a428-869f-4d6b-b6b7-0acb8fc6436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
